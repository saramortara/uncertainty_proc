---
title: "Understanding sources of uncertainty in Ecological Niche Models"
author: "Sara Mortara"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    theme: cosmo
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      cache = TRUE)
```

```{r pkg}
library(ggplot2)
library(dplyr)
library(data.table)
library(stringr)
library(wesanderson)
library(knitr)
library(raster)
library(rasterVis)
library(sp)
library(ggcorrplot)
library(kableExtra)
## for model selection
library(lme4)
library(bbmle)
library(MuMIn)
# functions
source("R/sun_map.R")
source("R/sun_plot.R")
source("R/read_sun.R")
source("R/model_selection.R")

```



```{r prep}
# species data
sp.trat <- list.files("results/uncertainty")
sp.df <- read.csv("data/epiphyte_species.csv", as.is = TRUE)
fam <- sort(table(sp.df$family), decreasing = TRUE)
fam.n <- as.numeric(fam)
# color palette
n.col <- 4
cores <-  wes_palette("Zissou1", n.col,
                      type = "continuous")
# algorithms
nome.algo <- c("Bioclim", "GLM", "Maxent", "Random Forest", "SVM")

# final stats
stats_rm <- read.csv("results/final_tables/stats_row_mean.csv")
stats_rmc <- read.csv("results/final_tables/stats_row_mean_cut.csv")
stats_rm$size <- as.factor(stats_rm$size)
stats_rmc$size <- as.factor(stats_rmc$size)

# occs
# species names
spp_names <- list.files("./data/epiphyte_occ/")
# listing .csv files
occ_files <- list()
for (i in 1:length(spp_names)) {
  occ_files[[i]] <- list.files(path = paste0("./data/epiphyte_occ/", spp_names[i]),
                               pattern = '.*0.csv',
                               full.names = TRUE)
}
# reading csv files
occ_data <- lapply(unlist(occ_files),
                   fread, sep = ',')

occ_n <- sapply(occ_data, nrow)[stringr::str_detect(sp.trat, "_N_100")]
sp.df$N <- occ_n 
```


# Goal

Uncertainty is a neglected topic in ecological niche modeling. Our aim is to propose an alternative metric to quantify uncertainty in ecological niche models (ENMs). We explored sources of uncertainty in ecological niche models built from different experimental species. Biotic data was generated from empirical occurrence data of epiphyte species from the Atlantic Forest. We used experimental scenarios (sample size, spatial autocorrelation) to define biotic data and two environmental dimensionality scenarios (low and high dimensionality). We defined undertainty as the difference between maximum and minimum suitability estimatated from each partition. From this measure, we derived two metrics to evaluate where uncertainty comes from. We then used regressions to better understand sources of uncertainty.

# Methods

## Biotic variables

We are using occurrence data from epiphyte species from the Atlantic Forest published in the database by Ramos et al. (2019) *Ecology*. We selected only Angiosperm epiphytes endemic to the Atlantic Forest (according to Brazilian Flora 2020 -- the official Brazilian plant list). We are using occurrence data from 32 species belonging to Bromeliaceae (N=`r fam.n[1]`), Orchidaceae (N=`r fam.n[2]`), Cactaceae (N=`r fam.n[3]`), Gesneriaceae (N=`r fam.n[4]`) Melastomataceae (N=`r fam.n[5]`) and Piperaceae (N=`r fam.n[6]`) families (Table 1). 

```{r tab.sp}
### adicionar o N na tabela
sp.list <- sp.df$sp
sp.df$sp <- gsub("_", " ", sp.df$sp)
sp.df$sp <- paste0("*", sp.df$sp, "*")
kable(sp.df[-27,],
      col.names = c("Family", "Species", "N of occurrences"),
      caption = "Table 1. List of species included in this report.")
```


## Abiotic variables

We are using environmental variables from World Clim at 2.5$^{\circ}$ for South America. We selected all variables except: Mean Temperature of Wettest Quarter (BIO08), Mean Temperature of Driest Quarter (BIO09), Precipitation of Warmest Quarter (BIO18), Precipitation of Coldest Quarter (BIO19). We performed a PCA with all environmental variables to build two environmental scenarios: low and high dimensionality. Low dimensionality scenario is represented by the first four PCA axes (> 90% of variance), whereas the high dimensionality scenario is represented by the eight fisrt axes (> 98% of variance).

## Experimental design

From 32 epiphyte species we generated different experimental scenarios with different sample sizes (10, 20, 50 and 100% of all occurrence data points) and different patterns of spatial aggregation (clump and no clump). We used the `DistanceFilter` function from ENMGadgets R package to exclude spatial aggregated data at a distance of 0.11785113 km (= 2 x pixel width). Therefore, for each species we generated samples of occurrence data with four sample sizes andtwo aggretation data, totalizing `r length(sp.trat)` treatments.  For each treatment we modeled ecological niche using the following algorithms: `r nome.algo`. So far, we only run models for the low dimensionality scenario. We separed data into four partitions and generated 1,000 pseudo absences around a buffer based on mean distance from points, cutting with a mean distance for the exclusion buffer. 

## Uncertainty metrics
We defined two metrics to quantify uncertainty based on the suitability vs. uncertainty relationship. We defined uncertainty as the difference between maximum and minimum suitability estimate per pixel. Then, we classified the quality of suitability values based on the amount of uncertainty they held. Good quality suitability values were low suitability values (< first quantile) and high suitability values (> third quantile) with low uncertainty (< first quantile, Figure 1). Bad quality suitability values were low and high suitability values with high uncertainty. Points in between the first and third quantites were not considered in the metrics. Our uncertainty metrics were therefore calculated based on the proportion of good quality suitability values from total (M1), from bad points (M2) and from the classified points (M3). Given that M1 and M3 fitted to a binomial distribution, they were adopted for in the regression analysis. 

${M1 = \frac{N_{good~points}}{N_{total}}}$

${M2 = \frac{N_{good~points}}{N_{bad~points}}}$

${M3 = \frac{N_{good~points}}{N_{good~points + bad~points}}}$

```{r m.example}
set.seed(42)
sun_plot(data = data.frame(a = rnorm(1000, mean = 0.5, sd = 0.15), 
                          b = rnorm(1000, mean = 0.5, sd = 0.15)), 
         title = "")

```

Figure 1. Classification of suitability values based on its relation with uncertainty values. Dotted lines represent first and third quantile of x and y axis.

## Statistical analysis

In order to understand how the proposed metrics reflect model uncertainty, we fitted a generalized linear mixed model (GLMM) with a binomial response error to model the variation in uncertainty metrics (M1 and M3). We included clumping, sample size and algorithm as fixed effects and species as a random effect. We build models wuth all additive combinations of the fixed effects (Table 2). We confronted all models simoultaneosly by using model selection. Best model was selected based on Akaike Information Criterion (AIC) and models with ${\Delta}$AIC ${\leq}$ 2 were considered equally plausible. In addition, we used partial R${^2}$ (sensu Nakagawa) to quantify relative contribution of fixed (controled experiment variables) and random effects (species identities). 

```{r mod.table}
# models included in the analysis
fixed = c(" ~ 1", 
          "algorithm", "clump", "size", 
          "clump + size", 
          "clump + algorithm",
          "size + algorithm", 
          "clump + size + algorithm")
mods <- data.frame(Models = paste0("m", sprintf("%02d", 0:7)), 
                   Fixed = fixed)
kable(mods, 
      col.names = c("Models", "Fixed effects"), 
      caption = "Table 2. Models used in the regression analysis")

```


# Results

Legends

- Clumping:
  - C = clump
  - N = no clump

- Sample size:
  - 010 = 10% sample size
  - 020 = 20% sample size
  - 050 = 50% sample size
  - 100 = 100% sample size

- Uncertainty metrics
  - M1 = N of good points/N total
  - M2 = N of good points/N bad points
  - M3 = N of good points/(N bad points + N of good points) 
  

## Understanding model metrics

### AUC values for each treatment

```{r auc}
ggplot(stats_rm, aes(x = algorithm, y = AUC, fill = size)) +
  geom_boxplot() +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  theme_classic()
```


### TSS values for each treatment

```{r tss}
ggplot(stats_rm, aes(x = algorithm, y = TSS, fill = size)) +
  geom_boxplot() +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  theme_classic()
```

### pROC values for each treatment

All pROC values are significant. We should check this. 

```{r proc}
ggplot(stats_rm, aes(x = algorithm, y = pROC, fill = size)) +
  geom_boxplot() +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  theme_classic()
```


### Omission values for each treatment

```{r omission}
ggplot(stats_rm, aes(x = algorithm, y = omission, fill = size)) +
  geom_boxplot() +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  theme_classic()
```

## Understanding overall uncertainty

Remarks: While TSS and AUC appear to have a positive relation to sample size, omission, pROC and uncertainty metrics were more robust to variation in sample size. Uncertainty metrics kept similar mean values among sample size, however variation is higher at low sample sizes than high sample sizes. 

### M1 = good/total

```{r m1}
ggplot(stats_rm, aes(x = algorithm, y = M1, fill = size)) +
  geom_boxplot() +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  theme_classic()
```

### M2 = good/bad

```{r m2}
ggplot(stats_rm, aes(x = algorithm, y = M2, fill = size)) +
  geom_boxplot() +
  labs(y = "M2 (log)") +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  scale_y_log10() +
  theme_classic()
```

### M3 = good/(good + bad)

```{r m3}
ggplot(stats_rm, aes(x = algorithm, y = M3, fill = size)) +
  geom_boxplot() +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  theme_classic()
```



## Correlation among all metrics

```{r cor}
metrics <- stats_rm[,c('AUC', 'TSS', 'pROC', 'M1', 'M3', 'omission')]
correlation <- cor(metrics, use = 'complete.obs')

ggcorrplot(correlation, 
           hc.order = TRUE, 
           type = "lower",
           lab = TRUE, 
           legend.title = "Correlation", 
           colors = c("#3B9AB2", "white", "#F21A00"))
  
```

## Variance partitioning of uncertainty metrics with GLMMs

```{r run.models}
stats_rm$sp.gen <- paste(stats_rm$genus, stats_rm$epithet, sep = "_")
stats_rm$good_bad <- stats_rm$good + stats_rm$bad 

# building models w/ function
mod_m1 <- mod_sel(data = stats_rm, 
                  m = "M1")

mod_m3 <- mod_sel(data = stats_rm, 
                  m = "M3")

# showing predictors
m1_preds <- broom::tidy(mod_m1$mods$m01, conf.int = TRUE, exponentiate = TRUE) %>%
    mutate(Model = "M1")
           # estimate = exp(estimate), 
           # conf.low = exp(conf.low), conf.high = exp(conf.high))
m3_preds <- broom::tidy(mod_m3$mods$m01, conf.int = TRUE, exponentiate = TRUE) %>%
     mutate(Model = "M3")
    #        estimate = exp(estimate), 
    #        conf.low = exp(conf.low), conf.high = exp(conf.high))

ors <- bind_rows(m1_preds[1:9,], m3_preds[1:9,])

```

```{r aic.tab}
aic_tab <- bind_rows(mod_m1$aic_tab, mod_m3$aic_tab)
Metric <- rep(c("M1", "M3"), each = 8)
kaic <- kable(cbind(Metric, aic_tab))
collapse_rows(kaic)
```



```{r odds.ratio}
#dodger <- position_dodge(width = 0.3)
# Elements like pointrange and position_dodge only work when the outcome
#   is mapped to y, need to go through with OR set as y then flip at the
#   end
por <- ggplot(ors, aes(y = estimate, x = term, colour = Model)) +
  geom_point(shape = 1) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high),
                  #position = dodger,
                  width = .1) +
  geom_hline(yintercept = 1.0, linetype = "dotted", size = 1) +
  scale_colour_manual(values = c("grey20", "grey60")) +
  #scale_y_log10() +
  labs(y = "Odds ratio", x = "Effect", colour = "Metric") +
  coord_flip() +
  theme_bw() 
  

por

```

## Suitability vs. uncertainty plots

### Looking at C 100 species

#### Bioclim C_100

```{r bio100, fig.width=3, fig.height=2.5}
sp.list <- sp.list[-27]
bio_100 <- read_sun(algo = "bioclim", 
                    cod = "C_100", 
                    type = "raw_mean")

  for (i in 1:length(bio_100)) {
sun_plot(bio_100[[i]], 
         title = sp.list[i])
}
```


#### GLM C_100

```{r glm100, fig.width=3, fig.height=2.5}
glm_100 <- read_sun(algo = "glm", 
                    cod = "C_100", 
                    type = "raw_mean")

for (i in 1:length(glm_100)) {
sun_plot(glm_100[[i]], 
         title = sp.list[i])
}
```

#### Maxent C_100

```{r max100, fig.width=3, fig.height=2.5}
max_100 <- read_sun(algo = "maxent", 
                    cod = "C_100", 
                    type = "raw_mean")

for (i in 1:length(max_100)) {
sun_plot(max_100[[i]], 
         title = sp.list[i])
}
```

#### Random Forest C_100

```{r rf100, fig.width=3, fig.height=2.5}
rf_100 <- read_sun(algo = "rf", 
                    cod = "C_100", 
                    type = "raw_mean")

for (i in 1:length(rf_100)) {
sun_plot(rf_100[[i]], 
         title = sp.list[i])
}
```

#### Support Vector Maxines C_100

```{r svm100, fig.width=3, fig.height=2.5}
svm_100 <- read_sun(algo = "svmk", 
                    cod = "C_100", 
                    type = "raw_mean")

for (i in 1:length(svm_100)) {
sun_plot(svm_100[[i]], 
         title = sp.list[i])
}
```


```{r map1}
# algo <- "bioclim"
# dir <- "models_low"
# sp <- sp.trat[1]
# occ <- occ_data[[1]]
# 
# sun_map(algos = algo, 
#         dir = dir, 
#         sp = sp,
#         occ = occ)
```

