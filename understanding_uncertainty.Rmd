---
title: "Understanding sources of uncertainty in Ecological Niche Models"
author: "Sara Mortara"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    theme: cosmo
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      cache = TRUE)
```

```{r pkg}
library(ggplot2)
library(dplyr)
library(data.table)
library(stringr)
library(wesanderson)
library(knitr)
library(raster)
library(rasterVis)
library(sp)
library(ggcorrplot)
## for model selection
library(lme4)
library(bbmle)
library(MuMIn)
# functions
source("R/sun_map.R")
source("R/sun_plot.R")
source("R/read_sun.R")
```



```{r prep}
# species data
sp.trat <- list.files("results/uncertainty")
sp.df <- read.csv("data/epiphyte_species.csv", as.is = TRUE)
fam <- sort(table(sp.df$family), decreasing = TRUE)
fam.n <- as.numeric(fam)
# color palette
n.col <- 4
cores <-  wes_palette("Zissou1", n.col,
                      type = "continuous")
# algorithms
nome.algo <- c("Bioclim", "GLM", "Maxent", "Random Forest", "SVM")

# final stats
stats_rm <- read.csv("results/final_tables/stats_row_mean.csv")
stats_rmc <- read.csv("results/final_tables/stats_row_mean_cut.csv")
stats_rm$size <- as.factor(stats_rm$size)
stats_rmc$size <- as.factor(stats_rmc$size)

# occs
# species names
spp_names <- list.files("./data/epiphyte_occ/")
# listing .csv files
occ_files <- list()
for (i in 1:length(spp_names)) {
  occ_files[[i]] <- list.files(path = paste0("./data/epiphyte_occ/", spp_names[i]),
                               pattern = '.*0.csv',
                               full.names = TRUE)
}
# reading csv files
occ_data <- lapply(unlist(occ_files),
                   fread, sep = ',')
```


# Goal

Our main goal is to quantify uncertainty in ecological niche models (ENMs). First, we will explore uncertainty in ecological niche models of epiphyte species from the Atlantic Forest using different algorithms and experimental scenarios (sample size, spatial autocorrelation and dimensionality). 

# Methods

## Biotic variables

We are using occurrence data from epiphyte species from the Atlantic Forest published in the database by Ramos et al. (2019) *Ecology*. We selected only Angiosperm epiphytes endemic to the Atlantic Forest (according to Brazilian Flora 2020 -- the official Brazilian plant list). We are using occurrence data from 33 species belonging to Bromeliaceae (N=`r fam.n[1]`), Orchidaceae (N=`r fam.n[2]`), Cactaceae (N=`r fam.n[3]`), Gesneriaceae (N=`r fam.n[4]`) Melastomataceae (N=`r fam.n[5]`) and Piperaceae (N=`r fam.n[6]`) families (Table 1). 

```{r tab.sp}
### adicionar o N na tabela
sp.list <- sp.df$sp
sp.df$sp <- gsub("_", " ", sp.df$sp)
sp.df$sp <- paste0("*", sp.df$sp, "*")
kable(sp.df[-27,],
      col.names = c("Family", "Species"),
      caption = "Table 1. List of species included in this report.")
```


## Abiotic variables

We are using environmental variables from World Clim at 2.5$^{\circ}$ for South America. We selected all variables except: Mean Temperature of Wettest Quarter (BIO08), Mean Temperature of Driest Quarter (BIO09), Precipitation of Warmest Quarter (BIO18), Precipitation of Coldest Quarter (BIO19). We performed a PCA with all environmental variables to build two environmental scenarios: low and high dimensionality. Low dimensionality scenario is represented by the first four PCA axes (> 90% of variance), whereas the high dimensionality scenario is represented by the eight fisrt axes (> 98% of variance).

## Experimental design

From 33 species we generated different experimental scenarios with different sample sizes (10, 20, 50 and 100% of all occurrence data points) and different patterns of spatial aggregation (clump and no clump). We used the `DistanceFilter` function from ENMGadgets R package to exclude spatial aggregated data at 0.11785113 (= 2 x pixel width). For each treatment N=`r length(sp.trat)`  we modeled ecological niche using the following algorithms: `r nome.algo`. So far, we only run models for the low dimensionality scenario. We separed data into four partitions and generated 1,000 pseudo absences around a buffer based on mean distance from points, cutting with a minimum distance for the exclusion buffer. 

## Uncertainty metrics
We defined two metrics to quantify uncertainty based on the suitability vs. uncertainty relationship. We classified the quality of suitability values based on the amount of uncertainty they held. Good quality suitability values were low suitability values (< first quantile) and high suitability values (> third quantile) with low uncertainty (< first quantile). Bad quality suitability values were low and high suitability values with high uncertainty. Points in between the first and third quantites were not considered in the metrics. Our uncertainty metrics were therefore calculated based on the proportion of good quality suitability values from total (M1), from bad points (M2) and from the classified points (M3). Given that M1 and M3 fitted to a binomial distribution, they were adopted for in the regression analysis. 

${M1 = \frac{N_{good~points}}{N_{total}}}$
${M2 = \frac{N_{good~points}}{N_{bad~points}}}$
${M3 = \frac{N_{good~points}}{N_{good~points + bad~points}}}$

```{r m.example}
set.seed(42)
sun_plot(data = data.frame(a = rnorm(1000, mean = 0.5, sd = 0.15), 
                          b = rnorm(1000, mean = 0.5, sd = 0.15)), 
         title = "")

```


Figure 1. Classification of suitability values based on its relation with uncertainty values. 

## Statistical analysis

In order to understand how the proposed metrics reflect model uncertainty, we fitted a generalized linear mixed model (GLMM) with a binomial response error to model the variation in uncertainty metrics (M1 and M3). We included clumping, sample size and algorithm as fixed effects and species as a random effect. We build models wuth all additive combinations of the fixed effects (Table 2). We confronted all models simoultaneosly by using model selection. Best model was selected based on Akaike Information Criterion (AIC) and models with ${\Delta}$AIC ${\leq}$ 2 were considered equally plausible. In addition, we used partial R${^2}$ (sensu Nakagawa) to quantify relative contribution of fixed (controled experiment variables) and random effects (species identities). 

```{r mod.table}
# models included in the analysis
fixed = c(" ~ 1", 
          "algorithm", "clump", "size", 
          "clump + size", 
          "clump + algorithm",
          "size + algorithm", 
          "clump + size + algorithm")
mods <- data.frame(Models = paste0("m", sprintf("%02d", 0:7)), 
                   Fixed = fixed)
kable(mods, 
      col.names = c("Models", "Fixed effects"), 
      caption = "Table 2. Models used in the regression analysis")

```


# Results

Legends

- Clumping:
  - C = clump
  - N = no clump

- Sample size:
  - 010 = 10% sample size
  - 020 = 20% sample size
  - 050 = 50% sample size
  - 100 = 100% sample size

- Uncertainty metrics
  - M1 = N of good points/N total
  - M2 = N of good points/N bad points
  - M3 = N of good points/(N bad points + N of good points) 
  

## Understanding model metrics

### AUC values for each treatment

```{r auc}
ggplot(stats_rm, aes(x = algorithm, y = AUC, fill = size)) +
  geom_boxplot() +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  theme_classic()
```


### TSS values for each treatment

```{r tss}
ggplot(stats_rm, aes(x = algorithm, y = TSS, fill = size)) +
  geom_boxplot() +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  theme_classic()
```

### pROC values for each treatment

All pROC values are significant. We should check this. 

```{r proc}
ggplot(stats_rm, aes(x = algorithm, y = pROC, fill = size)) +
  geom_boxplot() +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  theme_classic()
```


### Omission values for each treatment

```{r omission}
ggplot(stats_rm, aes(x = algorithm, y = omission, fill = size)) +
  geom_boxplot() +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  theme_classic()
```

## Understanding overall uncertainty

Remarks: While TSS and AUC appear to have a positive relation to sample size, omission, pROC and uncertainty metrics were more robust to variation in sample size. Uncertainty metrics kept similar mean values among sample size, however variation is higher at low sample sizes than high sample sizes. 

### M1 = good/total

```{r m1}
ggplot(stats_rm, aes(x = algorithm, y = M1, fill = size)) +
  geom_boxplot() +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  theme_classic()
```

### M2 = good/bad

```{r m2}
ggplot(stats_rm, aes(x = algorithm, y = M2, fill = size)) +
  geom_boxplot() +
  labs(y = "M2 (log)") +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  scale_y_log10() +
  theme_classic()
```

### M3 = good/(good + bad)

```{r m3}
ggplot(stats_rm, aes(x = algorithm, y = M3, fill = size)) +
  geom_boxplot() +
  facet_grid(clump ~ .) +
  scale_fill_manual(values = cores) +
  theme_classic()
```



## Correlation among all metrics

```{r cor}
metrics <- stats_rm[,c('AUC', 'TSS', 'pROC', 'M1', 'M3', 'omission')]
correlation <- cor(metrics, use = 'complete.obs')

ggcorrplot(correlation, 
           hc.order = TRUE, 
           type = "lower",
           lab = TRUE, 
           legend.title = "Correlation", 
           colors = c("#3B9AB2", "white", "#F21A00"))
  
```

## Variance partitioning of uncertainty metrics with GLMMs

```{r mod.M1}
stats_rm$sp.gen <- paste(stats_rm$genus, stats_rm$epithet, sep = "_")
# f <- M1 ~ clump*size*algorithm + (1|sp.gen)
# m <- buildmer(f,data=stats_rm,direction='order',control=lmerControl(optimizer='bobyqa'))

m01 <- lmer(M1 ~ clump + size + algorithm + (1|sp.gen), 
             data = stats_rm, family = "binomial")
m02 <- lmer(M1 ~ clump + size + (1|sp.gen),
             data = stats_rm, family = "binomial")
m03 <- lmer(M1 ~ clump + algorithm + (1|sp.gen), 
             data = stats_rm, family = "binomial")
m04 <- lmer(M1 ~ size + algorithm + (1|sp.gen), 
             data = stats_rm, family = "binomial")
m05 <- lmer(M1 ~ size + (1|sp.gen), 
             data = stats_rm, family = "binomial")
m06 <- lmer(M1 ~ algorithm + (1|sp.gen), 
             data = stats_rm, family = "binomial")
m07 <- lmer(M1 ~ clump + (1|sp.gen), 
             data = stats_rm, family = "binomial")
m00 <- lmer(M1 ~ 1 + (1|sp.gen), 
             data = stats_rm, family = "binomial") 
AICtab(m01, m02, m03, m04, m05, m06, m07, m00, 
       base = TRUE, weights = TRUE)


```


```{r mod.M3}
m01b <- lmer(M3 ~ clump + size + algorithm + (1|sp.gen), 
             data = stats_rm, family = "binomial")
m02b <- lmer(M3 ~ clump + size + (1|sp.gen), 
             data = stats_rm, family = "binomial")
m03b <- lmer(M3 ~ clump + algorithm + (1|sp.gen), 
             data = stats_rm, family = "binomial")
m04b <- lmer(M3 ~ size + algorithm + (1|sp.gen),
             data = stats_rm, family = "binomial")
m05b <- lmer(M3 ~ size + (1|sp.gen), 
             data = stats_rm, family = "binomial")
m06b <- lmer(M3 ~ algorithm + (1|sp.gen), 
             data = stats_rm, family = "binomial")
m07b <- lmer(M3 ~ clump + (1|sp.gen), 
             data = stats_rm, family = "binomial")
m00b <- lmer(M3 ~ 1 + (1|sp.gen), 
             data = stats_rm, family = "binomial") 
AICtab(m01b, m02b, m03b, m04b, m05b, m06b, m07b, m00b, 
       base = TRUE, weights = TRUE)
```

## Suitability vs. uncertainty plots

### Looking at C 100 species

#### Bioclim C_100

```{r bio100, fig.width=3, fig.height=2.5}
sp.list <- sp.list[-27]
bio_100 <- read_sun(algo = "bioclim", 
                    cod = "C_100", 
                    type = "raw_mean")

  for (i in 1:length(bio_100)) {
sun_plot(bio_100[[i]], 
         title = sp.list[i])
}
```


#### GLM C_100

```{r glm100, fig.width=3, fig.height=2.5}
glm_100 <- read_sun(algo = "glm", 
                    cod = "C_100", 
                    type = "raw_mean")

for (i in 1:length(glm_100)) {
sun_plot(glm_100[[i]], 
         title = sp.list[i])
}
```

#### Maxent C_100

```{r max100, fig.width=3, fig.height=2.5}
max_100 <- read_sun(algo = "maxent", 
                    cod = "C_100", 
                    type = "raw_mean")

for (i in 1:length(max_100)) {
sun_plot(max_100[[i]], 
         title = sp.list[i])
}
```

#### Random Forest C_100

```{r rf100, fig.width=3, fig.height=2.5}
rf_100 <- read_sun(algo = "rf", 
                    cod = "C_100", 
                    type = "raw_mean")

for (i in 1:length(rf_100)) {
sun_plot(rf_100[[i]], 
         title = sp.list[i])
}
```

#### Support Vector Maxines C_100

```{r svm100, fig.width=3, fig.height=2.5}
svm_100 <- read_sun(algo = "svmk", 
                    cod = "C_100", 
                    type = "raw_mean")

for (i in 1:length(svm_100)) {
sun_plot(svm_100[[i]], 
         title = sp.list[i])
}
```


```{r map1}
# algo <- "bioclim"
# dir <- "models_low"
# sp <- sp.trat[1]
# occ <- occ_data[[1]]
# 
# sun_map(algos = algo, 
#         dir = dir, 
#         sp = sp,
#         occ = occ)
```

